{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a ><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Logo_INSA_Lyon_%282014%29.svg/langfr-2560px-Logo_INSA_Lyon_%282014%29.svg.png\"  width=\"200\" align=\"left\"> </a>\n",
    "<div style=\"text-align: right\"> <h3><span style=\"color:gray\"> Projet de recherche </span> </h3> </div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<h1><center>Data Acquisition</center></h1>\n",
    "<h2><center> <span style=\"font-weight:normal\"><font color='#e42618'> Parsing Morningstar (UK) to obtain Analyst Reports containing natural language and predictions</font>  </span></center></h2>\n",
    "\n",
    "\n",
    "<h3><center><font color='gray'>JONAS GOTTAL</font></center></h3>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Project scope</h4>\n",
    "\n",
    "- Obtaining financial research containing both a report in natural language and a quantifiable prediction on the underlying asset\n",
    "- Analyzing the underlying data and building predictive models (Large Language Models (LLMs)) based on natural language \n",
    "- Ultimately showing that the analysts prediction can be outperformed by application of LLM's \n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"text-decoration:none; margin-top: 30px; background-color:#F2F2F2; border-color:#720006\">\n",
    "    <span style=\"color:#720006\">\n",
    "    <ol>\n",
    "        <li><a href=\"#1\"> <span style=\"color:#720006;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Parsing of non-protected ID Table</span> </a></li>\n",
    "       <li><a href=\"#2\"> <span style=\"color:#720006;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Parsing of dedicated analyst reports</span> </a></li> \n",
    "       <ol>\n",
    "       <li><a href=\"#3\"> <span style=\"color:#720006;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Target Data Format</span> </a></li>\n",
    "       <li><a href=\"#4\"> <span style=\"color:#720006;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Connection Setup</span> </a></li>\n",
    "       <li><a href=\"#5\"> <span style=\"color:#720006;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Parsing</span> </a></li>\n",
    "       </ol>\n",
    "       <li><a href=\"#6\"> <span style=\"color:#720006;text-decoration:underline;text-decoration-color:#F2F2F2\" \n",
    "       >Depreciated Code</span> </a></li>\n",
    "    </ol>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "- ```Python 3.9.18``` (conda env)\n",
    "- ```pip freeze > requirements.txt```\n",
    "- ```conda env export > environment.yml```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing of non-protected ID Table <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# initialise variables\n",
    "URL_ID, Name, Sector, Analyst, Title, Rating, Date = \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# Define the regex pattern for the URL_ID\n",
    "pattern = r'id=([A-Za-z0-9]+)'\n",
    "\n",
    "with requests.Session() as session:\n",
    "    # define page size\n",
    "    r = session.get(\"https://www.morningstar.co.uk/uk/research/equities/page/1?PageSize=2000\")\n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')  \n",
    "\n",
    "    data = []\n",
    "    #whole table\n",
    "    for tr in soup.find('table').find_all('tr'):\n",
    "        i=0\n",
    "        #each row\n",
    "        for td in tr.find_all('td'):\n",
    "            # select each element in the row with a different pattern\n",
    "            if i==0:\n",
    "                URL_ID = re.search(pattern, td.find(\"a\").get(\"href\")).group(1)\n",
    "                Name = td.text.strip('\\n')\n",
    "                \n",
    "            if i == 1:\n",
    "                Sector= td.text\n",
    "            if i == 2:\n",
    "                Analyst = td.text\n",
    "            if i == 3:\n",
    "                Title = td.text\n",
    "            if i == 4:\n",
    "                Rating = td.findNext(\"div\").get(\"class\")[1][-1]\n",
    "            if i == 6:\n",
    "                Date= td.text\n",
    "            i+=1\n",
    "\n",
    "        data.append([URL_ID, Name, Sector, Analyst, Title, Rating, Date, str(time.strftime(\"%d/%m/%Y\"))])\n",
    "    data.pop(0) #remove header\n",
    "    df = pd.DataFrame(data, columns=['URL_ID','Name','Sector','Analyst', 'Title','StarRating','Date','ParseDate'])\n",
    "\n",
    "#save to csv\n",
    "df.to_csv('Data/Analyst Reports/morningstar_update.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consistency check\n",
    "len(df) == len(df[\"URL_ID\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing of dedicated analyst reports <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Data Format <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal: JSON File where the keys are ID or Index and the values the complete content of the website\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"0P00007XVV\": {\n",
    "        \"Index\": 0,\n",
    "        \"ParseDate\": \"01/11/2023\",\n",
    "        \"ID\": \"0P00007XVV\",\n",
    "        \"Title\": \"Ebos' Scale Has It Leading the Pack but Shares Screen as Expensive\",\n",
    "        \"CompanyName\": \"Ebos Group Ltd\",\n",
    "        \"TickerSymbol\": \"EBO\",\n",
    "        \"Rating\": 2,\n",
    "        \"ReportDate\": \"22/11/2023\",\n",
    "        \"AuthorName\": \"Shane Ponraj\",\n",
    "        \"price\": {\n",
    "            \"value\": 36.15,\n",
    "            \"currency\": \"NZD\",\n",
    "            \"date\": \"22/11/2023\"\n",
    "        },\n",
    "        \"FairPrice\": 30.5,\n",
    "        \"Uncertainty\": \"Medium\",\n",
    "        \"CostAllocation\": \"Exemplary\",\n",
    "        \"EconomicMoat\": \"Narrow\",\n",
    "        \"FinancialStrength\": \"\",\n",
    "        \"AnalystNote\": {\n",
    "            \"Date\": \"22/11/2023\",\n",
    "            \"Text\": [\n",
    "                \"paragraph1\",\n",
    "                \"paragraph2\",\n",
    "                \"paragraph3\"\n",
    "            ]\n",
    "        },\n",
    "        \"Bulls\": [\n",
    "            \"Bullet1\",\n",
    "            \"Bullet2\",\n",
    "            \"Bullet3\"\n",
    "        ],\n",
    "        \"Bears\": [\n",
    "            \"Bullet1\",\n",
    "            \"Bullet2\",\n",
    "            \"Bullet3\"\n",
    "        ],\n",
    "        \"ResearchThesis\": {\n",
    "            \"Date\": \"22/11/202\",\n",
    "            \"Text\": [\n",
    "                \"paragraph1\",\n",
    "                \"paragraph2\",\n",
    "                \"paragraph3\"\n",
    "            ]\n",
    "        },\n",
    "        \"MoatAnalysis\": \"Text\",\n",
    "        \"RiskAnalysis\": \"Text\",\n",
    "        \"ManagementAnalysis\": \"Text\",\n",
    "        \"Overview\": {\n",
    "            \"Profile\": \"Text.\",\n",
    "            \"FinancialStrength\": \"Text.\"\n",
    "        }\n",
    "    },\n",
    "    \"NEXTID\": {\n",
    "        \"Index\": 1,\n",
    "        ...\n",
    "        \n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection set-up by manually created cookies <a id=\"4\"></a>\n",
    "1. Login with credentials on [Morningstar](https://www.morningstar.co.uk/)\n",
    "1. Navigate to [List of equity reports](https://www.morningstar.co.uk/uk/research/equities) \n",
    "1. Activate network on browser in developer view \n",
    "1. Click on any item in List of reports\n",
    "1. Search for get GET package to that site and right-click ```copy as cURL```\n",
    "1. Paste on website [curlconverter](https://curlconverter.com/python/) to get cookies and headers for Python ```request``` package\n",
    "1. Paste below the cookies and headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = {\n",
    "    'cookies': 'true',\n",
    "    'BackBtn1_436033967': 'goBackCount=1&backButtonLabel=Back&backButtonLabelKey=&backButtonUrl=https://www.morningstar.co.uk/&backButtonLabelLangId=en-GB',\n",
    "    '__RequestVerificationToken': '980n4PbQ2vFVis0UiZUve0cewKXF7pkqXrrVhwezwvRjVKRy0bwJKhpUwykfpg0DBgRqGyPJIfuvt8nw9VOYEj1khAuqtJbsjpGFqhHpHqI1',\n",
    "    'ASP.NET_SessionId': 'mf50p3d1zvnavxkrksxr2z2g',\n",
    "    'PSI': 'S',\n",
    "    'RT_uk_BS': '+I7qdl0ruLBRlX6zrB8CNg==',\n",
    "    'RT_uk_CD': 'Z3FHV2s09k+33Ze4YWY5fw==',\n",
    "    'RT_uk_GH': 'NJw9lUqJYSARIFYxMhKSrY2uMBNWS/p4GKtwmEp1758=',\n",
    "    'RT_uk_GI': 'VPOteaTqG3hIo+smfM6kmSDFHhjzgnW1ZB5pCq59hH1aIpJMWbtD+yYHD0FIF5UE',\n",
    "    'RT_uk_MD': '0c6sp50S7Gs/558o5UXz+A==',\n",
    "    'RT_uk_MS': '8X3mhE0/kf6o/dIJeMo7TA==',\n",
    "    'RT_uk_PS': '6G2wEmwHr8HiGZATAp96Mw==',\n",
    "    'ad-profile': '%7b%22AudienceType%22%3a13%2c%22UserType%22%3a2%2c%22PortofolioCreated%22%3a0%2c%22IsForObsr%22%3afalse%2c%22NeedRefresh%22%3atrue%2c%22NeedPopupAudienceBackfill%22%3afalse%2c%22EnableInvestmentInUK%22%3a-1%7d',\n",
    "    'mstar': 'V6A49A4A32CCAE550739B42AFC92AC930C',\n",
    "    'RT_uk_LANG': 'en-GB',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Sec-Fetch-Site': 'same-site',\n",
    "    # 'Cookie': 'cookies=true; BackBtn1_436033967=goBackCount=1&backButtonLabel=Back&backButtonLabelKey=&backButtonUrl=https://www.morningstar.co.uk/&backButtonLabelLangId=en-GB; __RequestVerificationToken=980n4PbQ2vFVis0UiZUve0cewKXF7pkqXrrVhwezwvRjVKRy0bwJKhpUwykfpg0DBgRqGyPJIfuvt8nw9VOYEj1khAuqtJbsjpGFqhHpHqI1; ASP.NET_SessionId=mf50p3d1zvnavxkrksxr2z2g; PSI=S; RT_uk_BS=+I7qdl0ruLBRlX6zrB8CNg==; RT_uk_CD=Z3FHV2s09k+33Ze4YWY5fw==; RT_uk_GH=NJw9lUqJYSARIFYxMhKSrY2uMBNWS/p4GKtwmEp1758=; RT_uk_GI=VPOteaTqG3hIo+smfM6kmSDFHhjzgnW1ZB5pCq59hH1aIpJMWbtD+yYHD0FIF5UE; RT_uk_MD=0c6sp50S7Gs/558o5UXz+A==; RT_uk_MS=8X3mhE0/kf6o/dIJeMo7TA==; RT_uk_PS=6G2wEmwHr8HiGZATAp96Mw==; ad-profile=%7b%22AudienceType%22%3a13%2c%22UserType%22%3a2%2c%22PortofolioCreated%22%3a0%2c%22IsForObsr%22%3afalse%2c%22NeedRefresh%22%3atrue%2c%22NeedPopupAudienceBackfill%22%3afalse%2c%22EnableInvestmentInUK%22%3a-1%7d; mstar=V6A49A4A32CCAE550739B42AFC92AC930C; RT_uk_LANG=en-GB',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    # 'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Host': 'tools.morningstar.co.uk',\n",
    "    'Accept-Language': 'en-gb',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6.1 Safari/605.1.15',\n",
    "    'Referer': 'https://www.morningstar.co.uk/',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing <a id=\"5\"></a>\n",
    "\n",
    "### Partial URLs to combine to final URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url1 =  'https://tools.morningstar.co.uk/ukp/stockreport/default.aspx?Site=uk&id='\n",
    "\n",
    "url2 = '&tab=15&isreport=true&LanguageId=en-GB&SecurityToken='\n",
    "\n",
    "url3 ='%5D3%5D0%5DE0WWE$$ALL'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to parse website for each variable with beautiful soup with for loops in ```soup.find_all``` to not fail if item doesn't exist (still some try catch elements necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise variables\n",
    "variable_list = [\"ParseDate\", \"Title\", \"CompanyName\", \"TickerSymbol\", \"Rating\", \"ReportDate\", \"AuthorName\", \"Price\", \"Currency\", \"PriceDate\", \"FairPrice\", \"Uncertainty\",  \"EconomicMoat\", \"CostAllocation\", \"FinancialStrength\", \"AnalystNoteDate\", \"AnalystNoteList\", \"BullsList\", \"BearsList\", \"ResearchThesisDate\", \"ResearchThesisList\", \"MoatAnalysis\", \"RiskAnalysis\", \"CapitalAllocation\", \"Profile\", \"FinancialStrengthText\"]\n",
    "\n",
    "for item in variable_list:\n",
    "    globals()[item] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables(soup):\n",
    "    # Re-initialize the variables\n",
    "    variable_list = [\"ParseDate\", \"Title\", \"CompanyName\", \"TickerSymbol\", \"Rating\", \"ReportDate\", \"AuthorName\", \"Price\", \"Currency\", \"PriceDate\", \"FairPrice\", \"Uncertainty\",  \"EconomicMoat\", \"CostAllocation\", \"FinancialStrength\", \"AnalystNoteDate\", \"AnalystNoteList\", \"BullsList\", \"BearsList\", \"ResearchThesisDate\", \"ResearchThesisList\", \"MoatAnalysis\", \"RiskAnalysis\", \"CapitalAllocation\", \"Profile\", \"FinancialStrengthText\"]\n",
    "\n",
    "    for item in variable_list:\n",
    "        globals()[item] = \"\"\n",
    "    # initialze doesnt work for those two\n",
    "    ReportDate = \"\" \n",
    "    AnalystNoteDate = \"\"\n",
    "\n",
    "    #python today's dateÂ in format  DD/MM/YYYY\n",
    "    ParseDate = time.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    # Get the title\n",
    "    for wrapper in soup.find_all('div', {\"id\":\"AnalystResearch\"}): \n",
    "        Title = wrapper.findChild().text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    # Get the company name\n",
    "    for wrapper in soup.find_all('span', {\"class\":\"securityName\"}): \n",
    "        CompanyName = wrapper.text\n",
    "\n",
    "    # Get the ticker symbol\n",
    "    for wrapper in soup.find_all('span', {\"class\":\"securitySymbol\"}): \n",
    "        TickerSymbol = wrapper.text\n",
    "\n",
    "    # Get the rating\n",
    "    for wrapper in soup.find_all('span', {\"class\":\"securityRating\"}):\n",
    "        Rating = wrapper.findChild(\"img\")[\"alt\"][-1]\n",
    "\n",
    "    # Get the report date\n",
    "    try:\n",
    "        for wrapper in soup.find('h3', {\"id\":\"lnkAnalystNote\"}).find_next(\"span\"):  \n",
    "            ReportDate = wrapper.text\n",
    "    except:\n",
    "        ReportDate = \"\"\n",
    "        \n",
    "\n",
    "    # Get the author name\n",
    "    for wrapper in soup.find_all('div', {\"class\":\"author clearfix\"}): \n",
    "        for item in list(wrapper.children):\n",
    "                \n",
    "                # Get the text and split it by <br/>\n",
    "                text=item.get_text(separator='<br/>').split('<br/>')\n",
    "                # The second element is Name\n",
    "                AuthorName = text[0].replace('by ', '')\n",
    "\n",
    "    # Get the price and currency\n",
    "    for wrapper in soup.find_all('datapoint', {\"id\":\"Price\"}): \n",
    "        s = wrapper.text\n",
    "        # Split the string by space\n",
    "        values = s.split()\n",
    "        # Assign the values to variables\n",
    "        Price = values[0].replace(',', '')\n",
    "        Currency = values[1]\n",
    "\n",
    "    # Get the price date\n",
    "    for wrapper in soup.find_all('div', {\"id\":\"Price\"}): \n",
    "        s = wrapper.findChild().text\n",
    "        # Split the string by space\n",
    "        values = s.split()\n",
    "        # Assign the values to variables\n",
    "        PriceDate = values[1]\n",
    "\n",
    "    # Get the fair price\n",
    "    for wrapper in soup.find_all('datapoint', {\"id\":\"FairValueEstimate\"}):\n",
    "        s = wrapper.text\n",
    "        # Split the string by space\n",
    "        values = s.split()\n",
    "        # Assign the values to variables\n",
    "        FairPrice = values[0].replace(',', '')\n",
    "\n",
    "    # Get the uncertainty\n",
    "    for wrapper in soup.find_all('datapoint', {\"id\":\"Uncertainity\"}): \n",
    "        Uncertainty = wrapper.text\n",
    "\n",
    "    # Get the economic moat\n",
    "    for wrapper in soup.find('datapoint', {\"id\":\"EconomicMoat\"}):\n",
    "        EconomicMoat = wrapper.text  \n",
    "\n",
    "    # Get the cost allocation\n",
    "    for wrapper in soup.find('datapoint', {\"id\":\"Stewardship\"}):\n",
    "        CostAllocation = wrapper.text  \n",
    "\n",
    "    # Get the financial strength\n",
    "    try:\n",
    "\n",
    "        pattern = r'Grade_(\\w)'\n",
    "        for wrapper in soup.find('datapoint', {\"id\":\"FinancialHealthGrade\"}):   \n",
    "            FinancialStrength = re.search(pattern, wrapper.get('src')).group(1)\n",
    "    \n",
    "    except:\n",
    "        FinancialStrength = \"\"\n",
    "\n",
    "\n",
    "    # Get the analyst note date\n",
    "    try:\n",
    "\n",
    "        for wrapper in soup.find_all('h3', {\"id\":\"lnkAnalystNote\"}): \n",
    "            AnalystNoteDate = wrapper.findChild().text\n",
    "    except:\n",
    "        AnalystNoteDate = \"\"\n",
    "    \n",
    "    # Get the analyst note as list\n",
    "    AnalystNoteList = []\n",
    "    try:\n",
    "\n",
    "        for i in soup.find_all('h3', {\"id\":\"lnkAnalystNote\"}):\n",
    "            for sib in i.next_siblings:\n",
    "                if sib.name == 'p':\n",
    "                    AnalystNoteList.append(sib.text.encode('ascii', 'ignore').decode('ascii'))\n",
    "                    #print(sib.text)\n",
    "                elif sib.name == 'h2':\n",
    "                    #print (\"*****\")\n",
    "                    break\n",
    "        AnalystNoteList.pop() # remove \"go to top\"\n",
    "    except: \n",
    "        AnalystNoteList = []     \n",
    "    \n",
    "    # Get the bulls say as list\n",
    "    BullsList = []\n",
    "    for wrapper in soup.find('div', {\"id\":\"BullsView\"}).find_all(\"li\"):\n",
    "        BullsList.append(wrapper.text.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # Get the bears say as list\n",
    "    BearsList = []\n",
    "    for wrapper in soup.find('div', {\"id\":\"BearsView\"}).find_all(\"li\"):\n",
    "        BearsList.append(wrapper.text.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # Get the research thesis date\n",
    "    for wrapper in soup.find_all('div', {\"id\":\"lnkThesis\"}): \n",
    "        ResearchThesisDate = wrapper.findChild().findChild().text\n",
    "\n",
    "    # Get the research thesis as list\n",
    "    ResearchThesisList = []\n",
    "    for i in soup.find_all('div', {\"id\":\"lnkThesis\"}):\n",
    "        for child in i.findChildren():\n",
    "            if child.name == 'p':\n",
    "                ResearchThesisList.append(child.text.encode('ascii', 'ignore').decode('ascii'))\n",
    "    # remove \"go to top\"\n",
    "    ResearchThesisList.pop()  \n",
    "\n",
    "    # Get the moat analysis\n",
    "    for i in soup.find_all('div', {\"id\":\"lnkMoatAnalysis\"}):\n",
    "        MoatAnalysis = i.findChildren()[1].text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    # Get the risk analysis\n",
    "    for i in soup.find_all('div', {\"id\":\"lnkRisk\"}):\n",
    "        RiskAnalysis = i.findChildren()[1].text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    # Get the capital allocation\n",
    "    for i in soup.find_all('div', {\"id\":\"lnkManagement\"}):\n",
    "        CapitalAllocation = i.findChildren()[1].text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    # Get the profile\n",
    "    for i in soup.find_all('p', {\"id\":\"lnkProfile\"}):\n",
    "        Profile = re.sub(r'^Profile: ', '', i.text.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # Get the financial strength text\n",
    "    for i in soup.find_all('p', {\"id\":\"lnkFinancialHealth\"}):\n",
    "        FinancialStrengthText = re.sub(r'^Financial Strength: ', '', i.text.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # return the variables\n",
    "    return ParseDate, Title, CompanyName, TickerSymbol, int(Rating), ReportDate, AuthorName, float(Price), Currency, PriceDate, float(FairPrice), Uncertainty, EconomicMoat, CostAllocation, FinancialStrength, AnalystNoteDate, AnalystNoteList, BullsList, BearsList, ResearchThesisDate, ResearchThesisList, MoatAnalysis, RiskAnalysis, CapitalAllocation, Profile, FinancialStrengthText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to concatenate lists used in JSON to also dump flat data in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_strings(string_list):\n",
    "    # Use the join method to concatenate the strings in the list\n",
    "    result_string = ' '.join(string_list)\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final for-loop over all IDs in IDList to parse each website and append to both lists (list for pd Dataframe and dict for JSON)\n",
    "Attention: the run-time is significantly faster and generally more stable at night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/29 (0.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/29 (3.45%)\n",
      "Progress: 2/29 (6.9%)\n",
      "Progress: 3/29 (10.34%)\n",
      "Progress: 4/29 (13.79%)\n",
      "Progress: 5/29 (17.24%)\n",
      "Progress: 6/29 (20.69%)\n",
      "Progress: 7/29 (24.14%)\n",
      "Progress: 8/29 (27.59%)\n",
      "Progress: 9/29 (31.03%)\n",
      "Progress: 10/29 (34.48%)\n",
      "Progress: 11/29 (37.93%)\n",
      "Progress: 12/29 (41.38%)\n",
      "Progress: 13/29 (44.83%)\n",
      "Progress: 14/29 (48.28%)\n",
      "Progress: 15/29 (51.72%)\n",
      "Progress: 16/29 (55.17%)\n",
      "Progress: 17/29 (58.62%)\n",
      "Progress: 18/29 (62.07%)\n",
      "Progress: 19/29 (65.52%)\n",
      "Progress: 20/29 (68.97%)\n",
      "Progress: 21/29 (72.41%)\n",
      "Progress: 22/29 (75.86%)\n",
      "Progress: 23/29 (79.31%)\n",
      "Progress: 24/29 (82.76%)\n",
      "Progress: 25/29 (86.21%)\n",
      "Progress: 26/29 (89.66%)\n",
      "Progress: 27/29 (93.1%)\n",
      "Progress: 28/29 (96.55%)\n"
     ]
    }
   ],
   "source": [
    "# initialise dictionary and list\n",
    "JSONdict = {}\n",
    "data = []\n",
    "\n",
    "# loop through the ID list\n",
    "for i in range(len(df[\"URL_ID\"])):\n",
    "    #print the progress with string and percentage\n",
    "    print(\"Progress: \" + str(i) + \"/\" + str(len(df[\"URL_ID\"])) + \" (\" + str(round(i/len(df[\"URL_ID\"])*100, 2)) + \"%)\")\n",
    "    \n",
    "    #just because sometimes there are outages\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            # define the url\n",
    "            url = url1 + df[\"URL_ID\"][i] + url2 + df[\"URL_ID\"][i] + url3  \n",
    "\n",
    "            # get the response\n",
    "            response = requests.get(url,\n",
    "            \n",
    "                cookies=cookies,\n",
    "                headers=headers,\n",
    "            )\n",
    "\n",
    "            # parse the response\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')  \n",
    "\n",
    "            # get the variables\n",
    "            ParseDate, Title, CompanyName, TickerSymbol, Rating, ReportDate, AuthorName, Price, Currency, PriceDate, FairPrice, Uncertainty, EconomicMoat, CostAllocation, FinancialStrength, AnalystNoteDate, AnalystNoteList, BullsList, BearsList, ResearchThesisDate, ResearchThesisList, MoatAnalysis, RiskAnalysis, CapitalAllocation, Profile, FinancialStrengthText = get_variables(soup)\n",
    "\n",
    "            # create a dictionary\n",
    "            dict_item = {\n",
    "                    \"Index\": i,\n",
    "                    \"ParseDate\": ParseDate,\n",
    "                    \"ID\": df[\"URL_ID\"][i],\n",
    "                    \"Title\": Title,\n",
    "                    \"CompanyName\": CompanyName,\n",
    "                    \"TickerSymbol\": TickerSymbol,\n",
    "                    \"Rating\": Rating,\n",
    "                    \"ReportDate\": ReportDate,\n",
    "                    \"AuthorName\": AuthorName,\n",
    "                    \"Price\": {\n",
    "                        \"Value\": Price,\n",
    "                        \"Currency\": Currency,\n",
    "                        \"Date\": PriceDate\n",
    "                    },\n",
    "                    \"FairPrice\": FairPrice,\n",
    "                    \"Uncertainty\": Uncertainty,\n",
    "                    \"EconomicMoat\": EconomicMoat,\n",
    "                    \"CostAllocation\": CostAllocation,\n",
    "                    \"FinancialStrength\": FinancialStrength,\n",
    "                    \"AnalystNote\": {\n",
    "                        \"Date\": AnalystNoteDate,\n",
    "                        \"Text\": AnalystNoteList\n",
    "                    },\n",
    "                    \"Bulls\": BullsList,\n",
    "                    \"Bears\": BearsList,\n",
    "                    \"ResearchThesis\": {\n",
    "                        \"Date\": ResearchThesisDate,\n",
    "                        \"Text\": ResearchThesisList\n",
    "                    },\n",
    "                    \"MoatAnalysis\": MoatAnalysis,\n",
    "                    \"RiskAnalysis\": RiskAnalysis,\n",
    "                    \"CapitalAllocation\": CapitalAllocation,\n",
    "                    \"Overview\": {\n",
    "                        \"Profile\": Profile,\n",
    "                        \"FinancialStrength\": FinancialStrengthText\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # add the dictionary to the list\n",
    "            JSONdict[str(df[\"URL_ID\"][i])] = dict_item # new key-value pair\n",
    "            \n",
    "            # add the item to the list\n",
    "            data.append([ParseDate, Title, CompanyName, TickerSymbol, Rating, ReportDate, AuthorName, Price, Currency, PriceDate, FairPrice, Uncertainty, EconomicMoat, CostAllocation, FinancialStrength, AnalystNoteDate, concatenate_strings(AnalystNoteList), concatenate_strings(BullsList), concatenate_strings(BearsList), ResearchThesisDate, concatenate_strings(ResearchThesisList), MoatAnalysis, RiskAnalysis, CapitalAllocation, Profile, FinancialStrengthText])\n",
    "\n",
    "        except:\n",
    "            # wait two second and then retry\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "df = pd.DataFrame(data, columns=[\"ParseDate\", \"Title\", \"CompanyName\", \"TickerSymbol\", \"Rating\", \"ReportDate\", \"AuthorName\", \"Price\", \"Currency\", \"PriceDate\", \"FairPrice\", \"Uncertainty\",  \"EconomicMoat\", \"CostAllocation\", \"FinancialStrength\", \"AnalystNoteDate\", \"AnalystNoteList\", \"BullsList\", \"BearsList\", \"ResearchThesisDate\", \"ResearchThesisList\", \"MoatAnalysis\", \"RiskAnalysis\", \"CapitalAllocation\", \"Profile\", \"FinancialStrengthText\"])\n",
    "#save df as csv\n",
    "df.to_csv('Data/Analyst Reports/data_update.csv', index=True)\n",
    "\n",
    "# save the dictionary as json\n",
    "with open('Data/Analyst Reports/data_update.json', 'w') as outfile:\n",
    "    json.dump(JSONdict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to load in different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#load df from json file \n",
    "df = pd.read_json('data.json', orient='index')\n",
    "# load the csv file\n",
    "df = pd.read_csv('data.csv', index_col=0)\n",
    "# save as excel file\n",
    "df.to_excel('data.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discontinued code (failure) <a id=\"6\"></a>\n",
    "It would be more elegant to do the login with a dedicated payload but unfortunately the website re-directs to an error and it doesn't work with requests (maybe there is a selenium workaround)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree, html\n",
    "\n",
    "URL = \"https://www.morningstar.co.uk/uk/membership/Auth0CallbackManager.ashx\" \n",
    "\n",
    "baseURI= \"https://www.morningstar.co.uk/uk/research/equities\"\n",
    "\n",
    "payload = { \n",
    "\t\"email\": \"\", \n",
    "\t\"Login\": \"\" \n",
    "}\n",
    "\n",
    "\n",
    "with requests.Session() as session:\n",
    "    session.headers['User-Agent'] = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'\n",
    "\n",
    "    session.get(URL) \n",
    "    time.sleep(1)\n",
    "    r = session.post(URL, data=payload) \n",
    "    time.sleep(2)\n",
    "    url1 = \"https://tools.morningstar.co.uk/ukp/stockreport/default.aspx?Site=uk&id=\"\n",
    "    url2 = \"&tab=15&isreport=true&LanguageId=en-GB&SecurityToken=\"\n",
    "    url3 = \"]3]0]E0WWE$$ALL\"\n",
    "    for i in range(1):#len(IDList):\n",
    "        url = url1 + IDList[i] + url2 + IDList[i] + url3  \n",
    "        r = session.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')  \n",
    "        #dom = etree.HTML(str(soup)) \n",
    "        with open(\"report.html\", \"w\", encoding='utf-8') as file:\n",
    "            file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
